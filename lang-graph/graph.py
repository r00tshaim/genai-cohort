from typing import TypedDict #way to indicate the expected data types of variables
from dotenv import load_dotenv  # For loading environment variables
from typing import Literal
from pydantic import BaseModel, Field  # For defining data models
import os

from ollama import Client  # Import the ollama client

from langfuse.openai import OpenAI


# Load environment variables from .env file
load_dotenv()


# Get keys for your project from the project settings page
os.environ["LANGFUSE_PUBLIC_KEY"] = os.getenv("LANGFUSE_PUBLIC_KEY", "")
os.environ["LANGFUSE_SECRET_KEY"] = os.getenv("LANGFUSE_SECRET_KEY", "")
os.environ["LANGFUSE_HOST"] = os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")  # Default to US region


# http://localhost:11434/v1 - v1 versioned API root, introduced for consistency with OpenAI's API format.
#If you're using LangChain, LangGraph, or LangSmith, they usually expect OpenAI-style routes like /v1/chat/completions
OLLAMA_URL_V1 = "http://localhost:11434/v1"

# http://localhost:11434 - Ollama native endpoint
OLLAMA_URL = "http://localhost:11434"
# Initialize a separate Ollama client for direct chat completions
# This allows you to call the LLM for chat in the same way you were calling OpenAI
#ollama_chat_client = Client(host=OLLAMA_URL)

# Use Langfuse's OpenAI wrapper pointed at Ollama
client = OpenAI(
    base_url=OLLAMA_URL_V1,
    api_key="ollama",  # any placeholder
)


class State(TypedDict):
    user_message: str = Field(description="The message provided by the user.")
    ai_response: str = Field(description="The response generated by the AI for the specific query, if coding question then response will have code or explanation of it and if general question will have general response.")
    is_coding_question: bool = Field(description="A flag indicating whether the query is a coding question.")

# Schema
class DetectQueryResponse(BaseModel):
    is_coding_question_ai: bool

class CodingQuestionResponse(BaseModel):
    ai_response: str

class GeneralQuestionResponse(BaseModel):
    ai_response_general: str

def detect_query(state: State) -> State:
    """
    Detects if the user message is a coding question.
    This is a placeholder function that should be replaced with actual logic.
    """
    # Extract the user message from the state
    user_message = state['user_message']
    

    # Placeholder logic for detecting coding questions
    # if user_message and any(keyword in user_message.lower() for keyword in ["code", "function", "bug", "error", "syntax"]):
    #     state["is_coding_question"] = True
    # else:
    #     state["is_coding_question"] = False

    SYSTEM_PROMPT = f"""
    You are a helpful assistant that determines if a user message is related to coding or not.
    Your task is to analyze the user's message and classify it as a coding question or not.

    Return the response in specified JSON boolean only.
    """
    messages = [
        { "role": "system", "content": SYSTEM_PROMPT },
        { "role": "user", "content": user_message }
    ]

    # Call the Ollama LLM for chat completion using the ollama_chat_client
    # The model "gemma3:latest" is specified here, aligning with your Ollama setup.
    response = client.beta.chat.completions.parse(
        model="mistral:latest",
        messages=messages,
        response_format=DetectQueryResponse
    )

    print("🕵️ [detect_query] Response from LLM:", response.choices[0].message.content)

    # Parse the structured response
    parsed_response = response.choices[0].message.parsed

    state["is_coding_question"] = parsed_response.is_coding_question_ai

     
    if state["is_coding_question"]:
        state["ai_response"] = "This is a coding question."
    else:
       state["ai_response"] = "This is not a coding question."

    # For now, we assume every message is a coding question
    return state


def route_query(state: State) -> Literal["solve_coding_question", "solve_simple_question"]:
    """
    Routes the query based on whether it's a coding question or not.
    This is a placeholder function that should be replaced with actual routing logic.
    """

    if state["is_coding_question"]:
        # Route to coding-related processing
        return "solve_coding_question"
        
    else:
        # Route to general processing
        return "solve_simple_question"
    

def solve_coding_question(state: State) -> State:
    """
    Processes a coding question.
    This is a placeholder function that should be replaced with actual logic.
    """

    user_message = state['user_message']
    # Placeholder logic for solving coding questions
    SYSTEM_PROMPT = f"""
    You are a coding assistant that helps solve coding questions.
    Your task is to analyze the user's message and provide a solution.
    Return the response in specified JSON format.
    """
    messages = [
        { "role": "system", "content": SYSTEM_PROMPT },
        { "role": "user", "content": user_message }
    ]

    # Call the Ollama LLM for chat completion using the ollama_chat_client
    # The model "gemma3:latest" is specified here, aligning with your Ollama setup.
    response = client.beta.chat.completions.parse(
        model="mistral:latest",
        messages=messages,
        response_format=CodingQuestionResponse
    )

    print("💻 [solve_coding_question] Response from LLM:", response.choices[0].message.content)

    # Parse the structured response
    parsed_response = response.choices[0].message.parsed

    state["ai_response"] = parsed_response.ai_response
    return state

def solve_simple_question(state: State) -> State:
    """
    Processes a simple question.
    This is a placeholder function that should be replaced with actual logic.
    """

    # Placeholder logic for solving simple questions
    user_message = state['user_message']
    # Placeholder logic for solving coding questions
    SYSTEM_PROMPT = f"""
    You are a general assistant that helps answer simple questions.
    Your task is to analyze the user's message and provide a solution or guidance.
    Return the response in specified JSON format.
    """
    messages = [
        { "role": "system", "content": SYSTEM_PROMPT },
        { "role": "user", "content": user_message }
    ]

    # Call the Ollama LLM for chat completion using the ollama_chat_client
    # The model "gemma3:latest" is specified here, aligning with your Ollama setup.
    response = client.beta.chat.completions.parse(
        model="mistral:latest",
        messages=messages,
        response_format=GeneralQuestionResponse
    )

    print("🤖 [solve_simple_question] Response from LLM:", response.choices[0].message.content)

    # Parse the structured response
    parsed_response = response.choices[0].message.parsed
    
    state["ai_response"] = parsed_response.ai_response_general
    return state



from langgraph.graph import StateGraph, START, END

# Create a state graph to manage the flow of the application
graph_builder = StateGraph(state_schema=State)

# Define the nodes in the graph
graph_builder.add_node("detect_query", detect_query)
graph_builder.add_node("solve_coding_question", solve_coding_question)
graph_builder.add_node("solve_simple_question", solve_simple_question)
graph_builder.add_node("route_query", route_query)


# Define the edges in the graph
graph_builder.add_edge(START, "detect_query")
graph_builder.add_conditional_edges("detect_query", route_query)
graph_builder.add_edge("solve_coding_question", END)
graph_builder.add_edge("solve_simple_question", END)

# Compile the graph to finalize its structure
graph = graph_builder.compile()


# Use the Graph
def run_graph(user_message: str) -> State:
    """
    Runs the state graph with the provided user message.
    """
    # Initialize the state with the user message
    initial_state: State = {
        "user_message": user_message,
        "ai_message": "",
        "is_coding_question": False
    }

    #print("🚦 Running the graph with the initial state...")

    # Run the graph with the initial state
    final_state = graph.invoke(initial_state)

    return final_state

if __name__ == "__main__":


    # Example user message
    user_message = input("Enter your message: ")
    #user_message = "Hi how are you ?"  # Example message for testing
    # Ensure the user message is not empty
    if not user_message.strip():
        print("User message cannot be empty.")
        exit(1)

    # Run the graph with the user message
    result_state = run_graph(user_message)

    # Print the final state
    print("📝 Final State:", result_state)